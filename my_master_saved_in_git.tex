\documentclass[a4paper,11pt,twoside]{report}

% \usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}

\usepackage{amsmath, amsfonts, amsthm, latexsym}

\usepackage[final]{pdfpages}

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}

\linespread{1.15}
\allowdisplaybreaks

\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z~wcięciem
\setlength{\parindent}{5mm}

%--------------------------- Nasze rzeczy ------------------------

\usepackage{graphicx}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{float}
\usepackage{matlab-prettifier}
\usepackage{array}
\usepackage{multirow}
\usepackage{ wasysym }

\usepackage{longtable}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% \usepackage{natbib}
\usepackage{graphicx}
\usepackage{enumitem}

\newcommand\Tstrut{\rule{0pt}{11pt}}         % = `top' strut
\newcommand\Bstrut{\rule[0pt]{0pt}{0pt}}   % = `bottom' strut

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 


%--------------------------- ŻYWA PAGINA ------------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i~dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia

\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}


% ---------------- Nagłówki rozdziałów ---------------------

\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- Spis treści ---------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- Spisy tabel i~obrazków ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i~obrazki razem


% --------------------- Definicje, twierdzenia etc. ---------------


\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                          % Space above
{3ex}%                          % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother

% ----------------------------- POLSKI --------------------------------

\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}



% -------------------------- POCZĄTEK --------------------------


% --------------------- Ustawienia użytkownika ------------------

\newcommand{\tytul}{Korekcja błędów systemu rozpoznawania mowy dla transkrypcji opisów MR w języku polskim}
\renewcommand{\title}{ASR Error Correction for MRI transcriptions in polish language}
\newcommand{\type}{Master} % magisters, licencjac
\newcommand{\supervisor}{dr inż. Janusz Rafałko}



\begin{document}
\sloppy
\selectlanguage{english}

\includepdf[pages=-]{titlepage}
% \includepdf[pages=-]{podpis.pdf}

% TODO comment this and include podpis.pdf
\pagenumbering{gobble}
\newpage
\null

\vfill

\begin{center}
\begin{tabular}[t]{ccc}
............................................. & \hspace*{100pt} & .............................................\\
supervisor's signature & \hspace*{100pt} & author's signature
\end{tabular}
\end{center}



% ---------------------------- ABSTRAKTY -----------------------------
% W~PRACY PO POLSKU, NAPIERW STRESZCZENIE PL, POTEM ABSTRACT EN

{\selectlanguage{english}
\begin{abstract}

\begin{center}
\title
\end{center}

The paper describes design and implementation of Automatic Speech Recognition post-processing system for creating Magnetic Resonance Imaging reports in polish language. The goal of the system is to enable accurate speech to text transcription of reports using existing speech recognition solutions with lower accuracy.\\
% The paper describes implementation and usage of post-processing system for open-source speech recognition model Julius 
The system utilises existing open-source Julius ASR model and designed post-processing error correction subsystem to overcome problems of this task, such as: lack of training data, medical terminology and polish language complexity. The system uses N-gram model to correct hypothesis. Additionally it coverts orthographic text to International Phonetic Alphabet to better recognise matching parts of speech. The paper shows results of post-processing system by calculating word error rate metric before and after correction.\\
Created system can be used by radiologists for more efficient analyzing of Magnetic resonance images with ability to correct mistakes in report on the spot. The system can also be adapted to different domains without need of having database of recordings, but only using text data.

\noindent \textbf{Keywords:} natural language processing, speech to text transcription, audio analysis, n-gram, phonetic transcription

\end{abstract}
}

\null\thispagestyle{empty}\newpage

{\selectlanguage{polish}
\begin{abstract}

\begin{center}
\tytul
\end{center}

% TODO

Celem pracy jest stworzenie i zaimplementowanie systemu do korekcji błędów dla modelu rozpoznawania dźwięku przeznaczonego do transkrybcji opisów Rezonansu Magnetycznego (MR) w języku polskim. Celem systemu jest umożliwienie dokładnego rozpoznawania mowy korzystając z istniejących rozwiązań o mniejszej dokładności.

System wykorzystuje istniejący otwartoźródłowy model Julius oraz zaprojektowany system korekcji błędów do przezwyciężenia problemów związanych z zadaniem, między innymi: małej ilości danych, specjalistycznego słownictwa oraz złożoności języka polskiego. System korzysta z modelu N-gram w celu poprawy błędów w hipotezach wygenerowanych przez Julius. Dodatkowo wykorzystuje transkrypcję na Międzynarodowy Alfabet Fonetyczny aby lepiej dopasowywać do siebie źle rozpoznane fragmenty tekstu. W pracy zostały przedstawione wyniki za pomocą Współczynnika błędnych słów (word error rate) przed i po zastosowaniu systemu korekcji błędów.

Stworzone rozwiązanie może zostać użyte przez lekarzy radiologów do szybszego opisywania badań MR. Dodatkowo rozwiązanie może zostać łatwo dostosowane do innych domen za pomocą jedynie danych tekstowych, bez potrzeby posiadania bazy danych nagrań.


\noindent \textbf{Słowa kluczowe:} przetwarzanie języka naturalnego, analiza dźwięku, n-gram, transkrypcja fonetyczna
\end{abstract}
}


% --------------------- OŚWIADCZENIE -----------------------------------------


\null\thispagestyle{empty}\newpage

\null \hfill Warsaw, ..................\\

\par\vspace{5cm}

\begin{center}
Declaration
\end{center}

I hereby declare that the thesis entitled ,,\title '', submitted for the \type ~degree, supervised  by \supervisor , is entirely my original work apart from the recognized reference.
\vspace{2cm}

\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage

\null\thispagestyle{empty}\newpage


% ------------------- 4. Spis treści ---------------------
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}


% \newpage % JEŻELI SPIS TREŚCI MA PARZYSTĄ LICZBĘ STRON, ZAKOMENTOWAĆ 
% ALBO JAK KTOŚ WOLI WTEDY DWIE STRONY ODSTĘPU, DODAĆ \null\newpage
% TODO na razie są dwie strony, trzeba sprawdzić na końcu czy tak zostanie


% -------------- 5. ZASADNICZA CZĘŚĆ PRACY --------------------
\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11} % JEŻELI Z~POWODU DUŻEJ ILOŚCI STRON W~SPISIE TREŚCI SIĘ NIE ZGADZA, TRZEBA ZMODYFIKOWAĆ RĘCZNIE
% TODO na końcu

\chapter*{Introduction}
\markboth{}{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

% \chapter{Introduction}

Speech to text software to create MRI reports is widely used in English speaking countries (US more than 80\% - sourced by Nuance © October 2019), however in Poland such systems don’t have much recognition. Due to polish language complexity, medical terminology and lack of training data such systems are difficult to develop and, in most cases, doctors still rely on manual transcription. The aim of the work is to create software focused on medical terminology to help radiologists in creating MRI reports.\\

Currently creation of the MRI report by radiologist in Poland consists of radiologist dictating the case and passing the audio file to the transcriptionist. The transcriptionist creates preliminary report, which is then reviewed by radiologist, that is then saved as a final report. The main drawbacks of this solution are: long time of execution and errors in final report due to radiologist forgetting details of the MRI image before reviewing preliminary report. With automatic speech recognition system we can eliminate both of them as radiologist will get preliminary report immediately after finishing dictation. \\

Depending on the results of created systems it will be tested by doctors and opportunity to use created system in practice will be discussed.\\

Chapter 1. of this paper includes descriptions of related concepts that helped in designing system as well as key definitions. Chapter 2. focuses on related works and state of the art regarding error-correction systems for ASR. Chapter 3. describes data used during training and testing. Chapter 4. contains implementation details. Chapter 5. shows results and summary of the work. 


\chapter{Definitions}

\section{Automatic Speech Recognition}

Automatic speech recognition (in short ASR) is system for speech to text transcription. It takes speech sound-wave as an input and produces correlating text as an output. General usage of ASR system is shown in figure \ref{img:asrworkflow}. 

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/asr_diagram_1.png}
    \caption{ASR flow}
    \label{img:asrworkflow}
\end{figure}


Most ASR algorithms are based either on Hidden Markov models or Neural Networks. The algorithm chosen as baseline ASR for this paper is Julius, which is primarily based on context-dependent Hidden Markov models.

\section{Hidden Markov models}

Hidden Markov Model (HMM) is a statistical model in which the modeled system is assumed to be a Markov process with hidden states $Q=\{ q_1, q_2, ... \}$ \cite{HMM}. HMM requires that there is an observable process $O=\{ o_1, o_2, ... \}$ such as outcomes of $O$ are influenced by the outcomes of $Q$ in a known way. Since $Q$ cannot be observed directly, the goal is to learn about $Q$ by observing $O$. HMM has an additional requirement that the outcome of $O$ at time $t$ may be "influenced" exclusively by the outcome of $Q$ at $t$ and that the outcomes of $Q$ and $O$ at $t_0 < t$ must not affect the outcome of $O$ at $t$. 

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/HMM1.png}
    \caption{Hidden Markov Model}
    \label{img:HiddenMarkovModel}
\end{figure}

\noindent There are 2 sets of model parameters (figure \ref{img:HiddenMarkovModel}):
\vspace{-5mm}
\begin{itemize}
    \setlength\itemsep{-0.5em}
    \item $a_{i,j}$ denotes probability of switching state from $q_i$ to $q_j$
    \item $b_i(o_j)$ denotes probability of observing $o_j$ while at state $q_i$
\end{itemize}

\noindent Hidden Markov models can be characterized by three fundamental problems:
\vspace{-5mm}
\begin{enumerate}
    \setlength\itemsep{-0.5em}
    \item Learning - Given an observation sequence $O$ and set of states in the HMM, learn the HMM parameters.
    \item Likelihood - Given an observation sequence $O$, determine the likelihood $P(O)$
    \item Decoding - Given an observation sequence $O$, discover the best hidden state sequence $Q$.
\end{enumerate}

\noindent In Automatic Speech Recognition the first and third problem are used.

\section{Hidden Markov models in Automatic Speech Recognition}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/hmm_in_asr.png}
    \caption{HMM-based ASR flow}
    \label{img:HiddenMarkovModelInASR}
\end{figure}

Picture \ref{img:HiddenMarkovModelInASR} shows typical architecture of ASR based on Hidden Markov models \cite{HMMinASR}. Algorithm begins from the extraction of features from input speech waveform into fixed size acoustic vectors. Then his vector is used to find the most probable sequence of words spoken that could give obtained acoustic vector.\\
The acoustic model uses the fact that every word in the language consists of phonemes, which are the basic unit of sound. For polish language there is around 40 different phonemes \cite{PolishPhonemes}.\\
The pronunciation dictionary stores all possible pronunciations of recognisable words using phonemes from acoustic model. Different pronunciations of the same word can be caused for example by different dialect of the speaker or depending on emotions affecting speaker at the time of recording the sound.\\
Finally language model is typically N-gram word model which parameters are calculated using text corpus.

Hidden Markov model in such ASR architecture is used to determine sequence of phonemes based on feature vectors. In the most basic context-independent type of Hidden Markov model each phoneme is treated as the hidden state in the continuous density HMM with acoustic vectors as observations. 

The phonemes sequence obtained using HMM is then used to create words using pronunciation dictionary, which are then used to create whole sentences using language model.

A bit more advanced context-dependent HMM adds an assumption that phonemes around do affect the pronunciation of the phoneme itself, which means that its not enough to represent every phoneme with a single state. The idea is to split every phoneme into multiple variants depending on surrounding phonemes. This operation will greatly increase number of parameters of the model, that allows for more accurate speech recognition.  


\section{N-Gram Language Model}

N-gram model is a statistical model that tries to calculate probability of the word $w_n$ based on preceding words $w_1, w_2, ..., w_{n-1}$ in the sentence \cite{HMM, NGram}. In the most simple form it uses equation:
$$
P(w_n|w_1 w_2 ... w_{n-1}) = \frac{C(w_1 w_2 ... w_{n-1} w_n)}{C(w_1 w_2 ... w_{n-1})}
$$
where $C(h)$ denotes number of occurrences of $h$ in training data. For example if we want to calculate the probability of the word "dog" right after "my favourite animal is" then we would need to calculate number of occurrences of "my favourite animal is dog" and divide it by number of occurrences of "my favourite animal is". 

One of the biggest problems of N-gram model is that for $N>2$ this model require a much larger text corpus for reliable estimation of the language model. Additionally number of possible n-grams grows exponentially with the order, meaning that solutions to reduce number of N-grams saved and to smooth the distribution obtained by counting n-gram occurrences needed to be developed. One of such methods is Katz smoothing.

\section{Katz smoothing}

Katz smoothing \cite{NGram, Katz} is an smoothing technique that takes into consideration lower order N-grams in assessing probability of given word. It defines non-zero probability for the word $w_n$ after sequence $w_1, w_2, ..., w_{n-1}$ even if n-gram $w_1, w_2, ..., w_n$ doesn't exist in training data. \\
The algorithm redefines $C$ count function from the definition of n-gram $w_1 ... w_n$ as follows:
\begin{equation}
    C_{katz}(w_1 w_2 ... w_n) = 
    \begin{cases}
    C(w_1 w_2 ... w_n),& \text{if } C(w_1 w_2 ... w_n)>k \\
    d_r C(w_1 w_2 ... w_n),& \text{if } k \geq C(w_1 w_2 ... w_n) > 0 \\
    \alpha (w_1 w_2 ... w_{n-1}) P(w_n | w_2 ... w_n),& \text{if } C(w_1 w_2 ... w_n) = 0
    \end{cases}
\end{equation}

where $d_r$ is is called "discount ration" and is equal to the discount predicted by Good-Turing estimate $\frac{C(w_1 w_2 ... w_n)^*}{C(w_1 w_2 ... w_n)}$, $k$ is treshold usually set to $5$, and $\alpha$ is calculated using following equation:
$$
\alpha (w_1 w_2 ... w_{n-1}) = \frac{1-\sum_{w_n:C(w_1 w_2 ... w_n)>0}P_{katz}(w_n | (w_1 w_2 ... w_{n-1})}{\sum_{w_n:C(w_1 w_2 ... w_n)>0}P_{katz}(w_n | (w_2 w_3 ... w_{n-1})}
$$
Intuitively it works by reducing probability of "weak" n-grams with count lower that $k$ and using spare probability for n-grams without representation in training data.

\section{Levenshtein distance and sequence alignment}

One of the most common metrics calculating distance between two sequences is Levenstain distance. For the first time introduces by Vladimir Levenshtein this metric calculates minimal number of single-character edit operations required to transform first sequence into the second one. Edit operations in basic version of the algorithm are limited to insertions, deletions and substitution, although extended versions also allow transpositions of two adjacent characters.\\

One common generalisation of Levenstain distance is computed using Needelman-Wunsh algorithm. It is especially useful as it also provides alignment of sequences which describes order of edit operations required to modify first sequence into the second one. Additionally it provides ability to fine-tune parameters such as match score, mismatch penalty and gap penalty. In terms of mismatch and match scores common tactic includes providing similarity matrix which potentially can include different scores for different pairs of entities. For example in alignment of text mismatch penalty between vowels should be lower than between vowel and consonant.


\chapter{Related Work}

\section{Post-Processing Error Correction System for ASR}

Modern Automatic Speech Recognition systems are still error-prone and imperfect, especially when it comes to out-of vocabulary words or multi-language sentences. Additionally they usually aim to cover as broad part of language model as possible and don't use domain-specific constraints that could be helpful in increasing accuracy of the system. That's when post-processing error correction system are used. \\
One of the ideas is to treat ASR as a black-box \cite{ErrBlackBox} with clearly specified input and output, but without any knowledge of nor ability to modify its internal architecture. The main advantage of this approach is the ability to apply post-processing algorithm to different ASRs without any modifications. \\
Generalized flow of the system is shown on figure \ref{img:asrworkflow2}.

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/asr_diagram_2.png}
    \caption{ASR with error-correction system}
    \label{img:asrworkflow2}
\end{figure}

\section{Accuracy of Automatic Speech Recognition}

Accuracy of ASR is usually measured by using Word Recognition Ration (WER). This value is calculated as Levenstain distance between hypothesis and reference, but with whole words as single tokens. The exact formula is as follows:
$$
WER = \frac{\text{Number of (insertions + deletions + substitutions)}}{\text{Number of words in reference}}
$$


\section{SPEECHPP}

One of the first ASR error-correction systems that treated ASR as a black-box was SPEECHPP \cite{Fertility}. It reduces number of errors in final hypothesis by effectively refining and tuning the
vocabulary and language model. To achieve it creators of the algorithm used techniques from
statistical machine translation and statistical speech recognition in order to model the errors that ASR system made in their domain. SPEECHPP tries to find $w$ that maximises following expression 
$$
P(w'|w) \cdot P(w)
$$
where $w'$ is hypothesis obtained by original ASR and $w$ is predicted original sequence of words. $P(w)$ was calculated using simple word-bigram language model with backoff smoothing.  $P(w'|w)$ was calculated assuming independent word-for-word substitutions.
$$
P(w'|w) = \prod_i{P(w_i'|w_i)}
$$
Unfortunately such representation was too simple, as many ASR errors involved mismatches between different number of words in hypothesis and reference, so in the final version of algorithm the concept of "fertility" of the word was used. Value of fertility for a given word in ASR hypothesis denoted number of words in the reference that corresponds to this word. For example when the reference is equal to "TAKE A TRAIN FROM CHICAGO TO TOLEDO" and ASR hypothesis is equal to "TICKET TRAIN FROM CHICAGO TO TO LEAVE" then the word "TOLEDO" will have fertility equal to 2, as shown in the figure \ref{img:toledo}.

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/toledo.png}
    \caption{Fertility example}
    \label{img:toledo}
\end{figure}

The main drawback of this method is that it needs really big training data set with both audio and text data to create model responsible for calculating $P(w'|w)$ and distribution of fertility values per word.


\section{Rule-based error correction}

One of the ideas related to error correction in speech recognition is rule-based error correction. The most naive implementation of this idea includes number of skilled linguists writing list of rules in the form of regular expressions. In the "Improving speech recognition through text-based linguistic post-processing" \cite{ErrRules} authors described way of creating rules automatically based on ASR mistakes in training model. \\
In order to create such rules ASR was used on training set and every mismatch between hypothesis and reference was treated as potential rule $A->B$, where $A$ is sequence of words in hypothesis, $B$ is sequence of words in reference. Then each rule was then tested on training set and rules that decreased accuracy were rejected. Such algorithm led to most rules being rejected or having minor impact on resulting accuracy of recogniser.\\
More advanced version of algorithm introduced context-sensitive rules in form $XAY->XBY$ which allowed for more sophisticated rules and higher accuracy increased in final model. 


\section{N-gram based correction}

N-gram language models are used in error correction can be used as an improvement of ASR internal language model \cite{ErrNGram}. In the paper "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset" authors used 5-gram model created on Microsoft Web N‐Gram dataset. They detected phrases in ASR hypothesis that were not in their language model and tried to create the best replacement. \\
The algorithms proposed in the paper consists of 3 steps: error spotting, correction candidates generation and finally context sensitive correction of errors.\\
Error spotting is done by recognising words in ASR hypothesis that are not in language model. For every such word $w_i$ the candidates are chosen from all unigrams from language model that share at least one 2-gram character sequence with  word $w_i$. From the list of candidates top 8 words are chosen that share the largest number of 2-gram character sequences with word $w_i$.\\
The last step of algorithm uses preceding 4 words $w_{i-4}, w_{i-3}, w_{i-2}, w_{i-1}$ for context-sensitive error correction. For every candidate $c_i$ 5-gram $w_{i-4}, w_{i-3}, w_{i-2}, w_{i-1} c_i$ is searched in language model and candidate with highest count is selected.\\
Similar system can work with Bing Spelling Suggestion instead of Microsoft Web N‐Gram dataset with additional advantage of simplified generation of candidates \cite{ErrBing}.

\section{Utterance verification}

Manual transcription of large number of audio tracks always include some errors. Ideally every case of such transcription should be additionally verified. It is one of the reasons why Utterance Verification (UV) algorithms were developed. \\

UV algorithms try to measure the confidence of transcription based on differences between audio track and transcription. Recently developed method by Yoonjae Jeong and Hoon-Young Cho \cite{UVPhonemeRanking} works on phonetic representation of transcription and additionally generated speech features using additional Deep Neural Network model. It generates forced alignment between those two vectors and runs the likelihood
ratio test. After filtering overly mismatched parts of speech the method calculates newly proposed Average Phoneme Ranking (APR) to further filter incorrectly transcribed parts of audio. APR between script $t$ and audio $u$ is calculated using following formula:
$$
APR(t,u) = \frac{1}{N}\sum_{i=1}^N rank(p_i, f_i)
$$
where $p_i$ corresponds to $i$-th phoneme in $t$, $f_i$ is the speech features vector corresponding to the same phoneme in $u$ and $rank(p_i, f_i)$ is the phoneme recognition rank of $p_i$ for $f_i$. Lower APR indicates higher confidence score between text and audio. \\

% TODO?\\
% \begin{verbatim}
%   https://www.researchgate.net/profile/Yik-Cheung-Tam/publication/262308966_ASR_error_detection_using_recurrent_neural_network_language_model_and_complementary_ASR/links/0a85e5374730599cf6000000/ASR-error-detection-using-recurrent-neural-network-language-model-and-complementary-ASR.pdf?origin=publication_detail
% \end{verbatim}

Another Utterance Verification idea recently proposed is based on cross modal attention \cite{UVAttention}. The method includes two LSTM-based models capable of detecting features from audio file and transcription and then using those two vectors of information for mismatch detection. To additionally improve results cross-modal attention was used. \\

It is worth noting that both those algorithms follow similar high-level design of firstly changing both audio and text files to similar domain - feature vectors or phonemes - and then introducing score that determines whether audio and text match with each other - using APR metric or cross-modal attention.


\section{Phoneme similarity matrix}

Phoneme similarity matrix is entity used to assess similarity between pairs of phonemes. It can be used in different alignment algorithms for better quality results. Such matrix is often generated using phoneme features described by experts \cite{MatrixExpert1, MatrixExpert2}. One example is that values in similarity matrix are correlated to percentage of common features between two phonemes. Such methods give good results, but require expert knowledge. \\
In some cases \cite{MatrixSurvey} phoneme similarity matrix were created using result from surveys asking participants to recognise phonemes. Resulting matrices generally produced different results for people from different backgrounds and allowed for greater personalisation of alogrithms on person-by-person basis, but had worse results as unified matrix.\\
Finally \cite{MatrixBlosum} proposed solution to generate phoneme similarity matrix based on transcibed audio files in automatic way without expert knowledge. Algorithm is similar to the one used in generation of BLOSUM martices in bioinformatics. Cells in matrix are filled according to the formula:
$$ 
W(\alpha, \beta ) = \log \frac{p(\alpha, \beta ) + p(\beta, \alpha )}{p(\alpha )p(\beta )}
$$
where $p(\alpha, \beta )$ is the frequency of substitution phoneme $\beta$ with phoneme $\alpha$, and $p(\alpha)$ is the frequency of phoneme $\alpha$ in training data. Every substitution entry equal to zero is substituted with the smallest non-zero entry from the matrix to ensure that matrix definition is well-defined. 




\chapter{Database - MRI reports}

Database used for testing of proposed algorithm consists of 2000 reports from mixed MRI types including 400 reports regarding brain MRI. Each report is saved in written form in DOC format and as a recording in WAVE format. \\
In every recording during first seconds the type of MRI is specified, which means that during development of application I can limit myself to only brain MRI. In the theoretical production instance the application would firstly determine type of MRI for report and then use models correlated with obtained type.\\

\noindent There are two main problems with database:
\begin{enumerate}
    \item There are different expressions in written and recorded report. The transcriptionist used certain phrases which had the same meaning, but different words used as it was in recording. It means that even with perfect automatic speech recognition there is no possibility for 100\% accuracy.
    \item All recording have really poor audio quality. The voice is too quiet and there is too much noise for ASR to give any meaningful output. After some preprocessing with noise reduction and changing sound volume Julius was able to recognise most of the phonemes, although still wasn't able to correctly merge them into words. In all recordings initial performance of Julius was between $0\%$ and $10\%$ word recognition ratio. 
\end{enumerate}

For 10 entries in database there were created custom recordings using the same equipment and environmental noise, but without mismatches between text and audio file.

\chapter{Concept}

\section{High level overview of the solution}

Diagram of proposed solution is shown on figure \ref{img:methodoverwiew}. It consists of:
\begin{enumerate}
    \item Sound preprocessing - improve quality of audio to get better results from base Automatic Speech Recognition system;
    \item Automatic Speech Recognition system - generate initial ASR hypothesis based on audio file;
    \item Phonetisation - change orthographic representation of training data into its phonetic representation;
    \item N-gram model - model used in post-processing methods to improve quality of ASR hypothesis;
    \item Post-processing method - several iterations of methods will be discussed in following sections.
\end{enumerate}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/High-level_model.drawio.png}
    \caption{High level overview}
    \label{img:methodoverwiew}
\end{figure}

\section{Sound preprocessing}

Sound preprocessing was necessary step because of poor quality of audio in database. Without it Julius was able to recognise at most 1/10 of all phonemes in recording.\\
In order to increase amount of recognisable phonemes three operations were performed:
\begin{enumerate}
    \item Frame rate normalisation: all files were normalised to be saved as 16000 Hz one-channel WAVE files;
    \item Volume change: volume of audio was increased by 40dB;
    \item Noise reduction: low-pass filer and high-pass filer were used to decrease noise of audio. More sophisticated methods distort audio too much to be recognisable by ASR.
\end{enumerate}

\section{Julius}

Julius is a high-performance, small-footprint large vocabulary continuous speech recognition decoder software based on word N-gram and context-dependent HMM.\\
The algorithm is based on 2-pass tree-trellis search, which fully incorporates major decoding techniques such as tree-organized lexicon, 1-best / word-pair context approximation, rank/score pruning, N-gram factoring, cross-word context dependency handling, enveloped beam search, Gaussian pruning, Gaussian selection, etc. \\
In the work I'm using newest version which is additionally improved by using Deep Neural Network.


\section{Method overview}

Initial experiments shown that in original ASR hypothesis almost no words are recognised properly, which made all previously discussed methods unusable. Additionally most of methods required large amount of training data to train models, which is not a case in this situation. \\

In initial proposed method training audio files will not be used at all. Audio files will be used only during test phase of the algorithm for calculating Word Error Ration before and after post-processing system. \\

Algorithm will use N-gram language model based on speech corpus created from written reports. It will use phonetic language IPA to improve measuring distance between phrases and to allow for non-polish pronunciations of scientific terms. Phrases from the model will be aligned with phonetic representation of ASR hypothesis to create improved hypothesis based on modification of Levenstain distance.\\


\section{International Phonetic Alphabet}

International Phonetic Alphabet is an alphabetic system of phonetic notation capable of representing phonemes from every existing language. It uses ways of creating the sound by human as a base, which allow for representing every phoneme. One of principals of IPA is that it is using exactly one letter to represent one sound. This can be either simple letter, for example '$t$', or letter plus diacritics, for example '$t^h$', depending on how precisely we want to represent sounds. \\
In the algorithm every representation is simplified to reduce distance of similar phonemes. 

\section{Language model}
Algorithm uses 5-gram word language model build on speech corpus created from written reports. Reports are normalised using following rules:
\begin{itemize}
    \item Intro of the report is removed, as in recordings doctors tell only type of the MRI report, not the whole introduction;
    \item At the beginning of the report the <START> tag is inserted, at the end of the report the <END> tag is inserted;
    \item Numbers are substituted with <NUMBER> tag;
    \item Punctuation marks are removed, all extra whitespaces are removed;
    \item Report is translated into its phonetic representation
\end{itemize}
Each word in language model is translated into its phonetic form in IPA language. Translation of each word is saved in pronunciation dictionary in order to switch back to orthographic representation at the end of the algorithm.

\section{Post-processing method}

Several methods were designed and tested. Few of them are described in this section.\\
All methods use ASR original hypothesis, called 'hypothesis', and produce improved hypothesis, called 'fixed output'. 

\subsection{Naive method} \label{sec:met1}

Algorithm starts by creating phonetic representation of hypothesis. The created phonetisation is then simplified, which means that all diacritics are removed. It then tries to align all existing 5-grams with hypothesis using algorithm method discussed below and the best 5-grams are chosen to be part of fixed output.\\

Method creates 3 lists:
\begin{enumerate}
    \item 'Possible N-grams' list, which stores N-grams that could be used in next steps of algorithm;
    \item 'Used N-grams' list, which stores N-grams that will be included in fixed output;
    \item 'Hypothesis Assignment' list, with the same length as number of signs in hypothesis, where each cell has index of N gram in 'Used N-grams' list that covers corresponding sign in hypothesis.  
\end{enumerate}

'Possible N-grams' list are the best K of all N-grams from language model regarding alignment score. Value of K is chosen empirically based on data available.\\
The 'Hypothesis Assignment' list is filled by iterating over the best N-grams from 'Possible N-grams'. Method will assign N-gram to its location based on its best alignment if all corresponding cells are empty and then it will add it to 'Used N-grams' list. The process is shown in the figure \ref{img:method1alignemntsimple}. If they are not empty the method will try to overlap active N-gram with N-grams already in 'Used N-grams' and then split resulting phrases into 3 parts. The process is shown in the figure \ref{img:method1alignemntadv}.

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/method1alignemntsimple.png}
    \caption{Filling Hypothesis Assignment - simple case}
    \label{img:method1alignemntsimple}
\end{figure}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/method1alignemntadv.png}
    \caption{Filling Hypothesis Assignment - advanced case}
    \label{img:method1alignemntadv}
\end{figure}

At the end of the process all phrases from 'Used N-grams' list are concatenated based on their order in 'Hypothesis Assignment'. They are converted to their orthographic representation using pronunciation dictionary and returned as fixed output.\\

Problems with method:
\begin{itemize}
    \item Slow (5-10 minutes per testcase)
    \item Not too much room for improvements
\end{itemize}

\subsection{Method advanced 2} \label{sec:met2}

This method creates fixed output iteratively based on previously generated words and current position in ASR hypothesis. This way it can use more information stored in n-gram model, mainly probability of word based on preceding words. Additionally method is much faster than the previous one. The method operates using two main entities: currently built fixed output and position in original ASR hypothesis.\\

Fixed output is initiated as list containing only one element: <START> token. Position in ASR hypothesis is initiated with 0. \\
At every step of the algorithms alignment of every word in corpus is calculated at the position in original ASR hypothesis. Alignment algorithm is modified in a way to negate gap penalty at the end of ASR hypothesis. Alignment score is then multiplied by probability obtained from n-gram model. Example of the process is shown on figure \ref{img:method2example}.\\
The word with highest final score is taken as next word in fixed output. Position in ASR hypothesis is moved by number of letters in hypothesis aligned with the chosen word. Example of the process is shown on picture \ref{img:method2example2}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/method2_example.png}
    \caption{Probability and alignment scores}
    \label{img:method2example}
\end{figure}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=220pt]{images/method2_example_position.png}
    \caption{Determining next position - method 2}
    \label{img:method2example2}
\end{figure}

Final method is much faster than the previous one - around 10 seconds per testcase, but it produces worse results. Mistakes are caused mainly be wrongly assessing position at the end of the word and including parts of possible future words in current word alignment.

\subsection{Method advanced 3}

This method is based on method described in section \ref{sec:met2}, but is focused on better assessment of ending of words. As in previous method it operates on currently built fixed output and position in original ASR hypothesis. \\

The method instead of storing position of end of last word stores position of beginning of last word in currently built fixed output. It then aligns ASR hypothesis with concatenation of last word in fixed output, special character "\$" and word in corpus. Alignment method is modified to not penalise special character.\\

Probability scores, final scores and best word are calculated in the same way as in section \ref{sec:met2}. Next position is determined be position of special character in alignment of the chosen word. Example of the process is shown on picture \ref{img:method3example}.

\begin{figure}[H]{}
    \centering
    \includegraphics[width=220pt]{images/method3_example.png}
    \caption{Determining next position - method 3}
    \label{img:method3example}
\end{figure}

This modification significantly improved results of previous method by ensuring better estimation of beginnings of next words in alignment with hypothesis. Unfortunately results were still worse than in method described in section \ref{sec:met1} due to the fact that once wrong combination of words occurs in fixed output it is unlikely that it returns on the right track. 

\subsection{Method 4}

While previous methods explored only one path in graph of connections between words from n-gram model this method tries to follow multiple paths at once using dynamic programming.\\
The method creates operates on three main collections:
\begin{itemize}
    \item Alignment score matrix - containing alignment score of each word in corpus at every position of ASR hypothesis;
    \item Score matrix - containing final score of each word in corpus at every position of ASR hypothesis;
    \item Transition matrix - containing information which words is the most probable preceding word for every word in corpus at every position of ASR hypothesis;
    \item Word length matrix - containing information about estimated part of word already processed for every word in corpus at every position of ASR hypothesis, to not break words in half.
\end{itemize}

Score matrix is initialised with zeros, with one at position of token <START> and beginning of ASR hypothesis.\\
Transition matrix is filled with -1.\\
Word length matrix is initialised with zeros.\\

The algorithm processes each letter of hypothesis, which corresponds to one column in matrices, beginning with first letter. Firstly, probability of each word is assessed based on previous column of score matrix, transition matrix and word length matrix. It is further described in section \ref{sec:transition}.\\
Secondly, words are aligned with hypothesis at current position. Alignment modifications are described in section \ref{sec:alignment}. Word length matrix is updated if alignment score is higher than in alignment for previous ASR hypothesis position. \\
Finally probabilities are multiplied by alignment score.\\

At the end word with the highest score at the last position of ASR hypothesis is chosen and fixed output is build using the same algorithm as taking last N words in section \ref{sec:transition}. Fixed output is translated from phonetic representation by pronunciation dictionary and returned.


\subsection{Transition matrix and probabilities of next words} \label{sec:transition}

Transition matrix contains information which words is the most probable preceding word for every word in corpus at every position of ASR hypothesis. At every step of algorithm for every word the most probable sequence of N words is generated, with N being equal to length of N-grams in language model. Such sequences are generated in a following way:\\
\begin{enumerate}
    \item Sequence of preceding words is initiated with index of chosen word
    \item Position is initiated with current position in main algorithm
    \item While sequence of preceding words is shorter than N:
    \begin{enumerate}
        \item Get index of word from transition matrix at location of last word in sequence of preceding words and position.
        \item If said word is different than last word in sequence of preceding words, append it to the sequence.
        \item Reduce position by 1.
    \end{enumerate}
    \item Return reversed sequence of preceding words.
\end{enumerate}

Using sequence of preceding words N-gram model can calculate probability of next words. List of obtained probabilities is modified in a following way:
\begin{enumerate}
    \item If word length from word length matrix is higher than 1 then probability of said word not changing is fixed at 0.9. Probabilities of other words are reduced accordingly;
    \item If word length from word length matrix is higher than 0 then probability of said word not changing is fixed at 0.6. Probabilities of other words are reduced accordingly;
    \item Probabilities are multiplied by score value from score matrix.
\end{enumerate}
Obtained values are inserted in score matrix at next position of ASR hypothesis. Last word in sequence of preceding words is saved in transition matrix. In case of multiple sequences providing values for the same word the highest value is chosen.

\subsection{Partial alignment at fixed position} \label{sec:alignment}

In order to calculate alignment of word at fixed position in ASR hypothesis alignment algorithm was changed in following ways:
\begin{enumerate}
    \item ASR hypothesis is reduced to only contain twice the number of characters as query word, starting from fixed position. This way alignment procedure is faster while result should not change;
    \item Gap in ASR hypothesis that extends up to the last character is not penalised;
    \item Gap in ASR hypothesis that starts at the beginning is penalised twice;
    \item Alignment score is divided by length of query word. Value obtained in such way will be related to percentage of aligned characters in query word.
    \item Score is increased further by small factor proportional to length of query word. This way longer words will be prioritized in case of equal percentage of aligned characters.
\end{enumerate}

If obtained alignment score is lower than score already present in alignment score matrix then better alignment was found at earlier place and there is no need to modify anything.
Otherwise length of the word is calculated as number of signs in aligned ASR hypothesis up to the last gap. Word length matrix is filled in using obtained length. Additionally cells corresponding to positions in ASR hypothesis up to obtained word length in alignment score matrix are filled with obtained alignment score.

\section{Alignment improvements}

In order to improve quality of alignment phoneme similarity matrix was created. Due to no access to specialist knowledge in terms of phoneme similarity data-oriented approach based on algorithm used in created BLOSUM matrices was chosen. The algorithm works as follows:
\begin{enumerate}
    \item Let N be number of phonemes used by grapheme to phoneme (G2P) software
    \item For each entry in dataset that includes both text and audio data calculate alignment using basic Needleman-Wunsch algorithm between phonetic representation of text file and phonetised ASR hypothesis
    \item Create matrix $f[N \times N]$, where $f_{i,j}$ is equal to number of substitutions of $i$-th phoneme with $j$-th phoneme or vice versa. $f_{i,i}$ is equal to number of correctly aligned $i$-th phoneme.
    \item Create matrix $q[NN \times N]$ as normalised $f$ matrix - $q_{i,j} = \frac{f_{i,j}}{sum(f)}$. Matrix $q$ estimates observed probability of mismatches between $i$-th and $j$-th phonemes.
    \item Create vector $p[N]$, where $p_i$ is equal to number of $i$-th phonemes in the whole dataset divided by number of all phonemes in the dataset. Vector $p$ denotes probability of phonemes.
    \item Create matrix $e[NxN]$, where $e_{i,j}=2 \cdot p_i \cdot p_j$ if $i!=j$, $e_{i,i}=p_i \cdot p_i$. Matrix $e$ estimates expected probability of mismatches between $i$-th and $j$-th phonemes.
    \item Create matrix $s[NxN]$, where $s_{i,j} = 2 \cdot \log_2 \left( \frac{q_{i,j}}{e_{i,j}} \right)$
\end{enumerate}

Matrix $s$ is the final similarity matrix. TODO wszędzie gdzie nie da się policzyć logarytmu albo dzielenia wartości są wypełniane przez najmniejszą wartość w macierzy.
At the end it is converted into format accepted by alignment algorithm.

\section{Mismatch correction system}

Although post-processing methods work well on custom text examples without mismatches between text and audio tracks, they have much worse results on original testcases. That is why development of mismatch correction system is needed. Mismatch correction system is trained on training text and audio data and applied before main post-processing method. Updated architecture diagram of solution is presented on figure \ref{img:overview2}.

\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/High-level_model_2ver.drawio.png}
    \caption{High level overview - updated}
    \label{img:overview2}
\end{figure}

Mismatch correction system is divided into training and testing phase. During training phase firstly it iterates through every training example and detects parts of ASR hypothesis with low similarity between ASR hypothesis and reference. Similarity is calculated as percentage of correctly assigned characters in alignment. Example of such analysis is visualised on figure \ref{img:mismatch}. Each of detected pairs is converted to one rule transforming misrecognised part of ASR hypothesis into correct part of reference. Rule is then tested on randomly chosen training cases and is saved only if it increases alignment score of ASR hypothesis against reference. 



\begin{figure}[H]{}
    \centering
    \includegraphics[width=\textwidth]{images/rule_detection.png}
    \caption{Mismatches detection algorithm - detected parts are highlighted green}
    \label{img:mismatch}
\end{figure}



\chapter{Results}

Methods were tested on available 400 brain MRI reports. Dataset was split into 90\% (360) training data and 10\% (40) test data. Additionally for 10 entries in test data there were recorded additional custom audio files containing exactly the same words as in written report with similar quality of equipment as original audio.\\
The results are as follows:\\
For original audio base ASR achieved between 1-1.25 WER in every report in test data with average of 1.10. Detailed data is shown on chart \ref{img:wynikorghyp}. For custom recording base ASR achieved between 1.06-1.20 WER with mean equal to 1.11. Histogram of results is shown on chart \ref{img:wynikmyhyp}\\

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_org_hyp1.png}
    \caption{WER of original ASR hypothesis}
    \label{img:wynikorghyp}
\end{figure}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_my_hyp1.png}
    \caption{WER of original ASR hypothesis}
    \label{img:wynikmyhyp}
\end{figure}

\subsection{Method 1}

ASR with post-processing system implementing method 1 achieved 0.6-1.0 WER on most cases from original data with average of 0.85. The average improvement by post-processing system was 0.25 WER. Detailed data is shown on charts \ref{img:wynikorgfix} and \ref{img:wynikorgdif}. Histogram of execution time of the method is shown on chart TODO\\

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_org_fix1.png}
    \caption{WER of fixed output of post-processing system}
    \label{img:wynikorgfix}
\end{figure}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_org_dif1.png}
    \caption{Change in WER between ASR hypothesis and fixed output}
    \label{img:wynikorgdif}
\end{figure}

For custom recordings ASR with post-processing system achieved between 0.20 and 0.55 WER with mean equal to 0.38. The average improvement by post-processing system was 0.73 WER with the best improvement of 0.88. The results are represented on charts \ref{img:wynikmyhyp}, \ref{img:wynikmyfix} and \ref{img:wynikmydif}. Histogram of execution time of the method is shown on chart TODO\\

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_my_fix1.png}
    \caption{WER of fixed output of post-processing system}
    \label{img:wynikmyfix}
\end{figure}

\begin{figure}[H]{}
    \centering
    \includegraphics[width=100pt]{images/wynik_my_dif1.png}
    \caption{Change in WER between ASR hypothesis and fixed output}
    \label{img:wynikmydif}
\end{figure}

The results are quite good and will be used as entry point to result analysis of following methods.

\subsection{Method 2}




% -------------------- 6. Bibliografia -----------------------
% Bibliografia leksykograficznie wg nazwisk autorów
% Dla ambitnych - można skorzystać z~BibTeX-a

\bibliographystyle{unsrt}
\begin{thebibliography}{20}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}

\bibitem[1]{HMM} Daniel Jurafsky, James H. Martin, \emph{Speech and Language Processing, 3rd ed. draft }, 2021.
\bibitem[2]{HMMinASR} Mark Gales, Steve Young, \emph{The Application of Hidden Markov Models in Speech Recognition}, now publishers, 2008.
\bibitem[3]{PolishPhonemes} Wiktor Jassem, \emph{Polish}, Cambridge University Press, 2016.
\bibitem[4]{NGram} Christian Mandery, \emph{Distributed N-Gram Language Models: Application of Large Models to Automatic Speech Recognition}, Studienarbeit, 2011
\bibitem[5]{Katz} Stanley F. Chen, Joshua Goodman, \emph{An Empirical Study of Smoothing Techniques for Language Modeling}, 1998
% \bibitem[5]{Levenstain} В. И. Левенштейн, \emph{Двоичные коды с исправлением выпадений, вставок и замещений символов}, 	1965
\bibitem[6]{ErrBlackBox} K. Ringger, J. F. Allen, \emph{Error Correction Via a Post-Processor for Continuous Speech Recognition}, 1996
\bibitem[7]{Fertility} K. Ringger, J. F. Allen, \emph{A Fertility Channel Model for Post‐Correction of Continuous Speech Recognition}, 1996
\bibitem[8]{ErrNGram} Youssef Bassil, Paul Semaan, \emph {ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset}, Journal of Computing, 2012
\bibitem[9]{ErrBing} Youssef Bassil, Mohammad Alwani \emph {Post-Editing Error Correction Algorithm For Speech Recognition using Bing Spelling Suggestion}, International Journal of Advanced Computer Science and Applications, 2012
\bibitem[10]{ErrRules} Ronald Lloyd Brandow, Tomak Strzalkowski, \emph{Improving speech recognition through text-based linguistic post-processing}, 2000
\bibitem[11]{UVPhonemeRanking} Yoonjae Jeong, Hoon-Young Cho, \emph{Detecting mismatch between text script and voice-over using utterance verification basen on phoneme recognition ranking}, 2020
\bibitem[12]{UVAttention} Qiang Huang, Thomas Hain, \emph{Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention}, 2019
\bibitem[13]{MatrixExpert1} J. Nerbonne and W. Heeringa, \emph{Measuring Dialect Distance Phonetically}, 1997
\bibitem[14]{MatrixExpert2} M. Pucher, A. Türk, J. Ajmera, N. Fecher, \emph{Phonetic distance measures for speech recognition vocabulary and
grammar optimization,}, 2007
\bibitem[15]{MatrixSurvey}  [A. Cutler, A. Weber, R. Smits, and N. Cooper, \emph{Patterns of English phoneme confusions by native and non-native listeners}, 2004
\bibitem[16]{MatrixBlosum}  [Ben Hixon, Eric Schneider, Susan L. Epstein, \emph{Phonemic Similarity Metrics to Compare Pronunciation Methods}, 2011


% Phoneme Similarity Matrices to Improve Long Audio Alignment for Automatic Subtitling
% https://hal.archives-ouvertes.fr/hal-01099239/document
% DETECTING MISMATCH BETWEEN TEXT SCRIPT AND VOICE-OVER USING UTTERANCE VERIFICATION BASED ON PHONEME RECOGNITION RANKING
% https://arxiv.org/pdf/2003.09180.pdf
% phoneme similarity matrices for Long audio alignment
% https://sites.google.com/site/similaritymatrices/publications

% ASR Post-Correction for Spoken Dialogue Systems
% Based on Semantic, Syntactic, Lexical and Contextual
% Information
% (rule based correction system)
% idk what was that, but it was in the mail...


% \bibitem[9]{ffmpeg} \emph{https://ffmpeg.org/}
% \bibitem[10]{parselmouth} \emph{https://parselmouth.readthedocs.io/}
% \bibitem[11]{praat} \emph{https://www.fon.hum.uva.nl/praat/}
% \bibitem[12]{pydub} \emph{http://pydub.com}
% \bibitem[13]{flask} \emph{https://flask.palletsprojects.com/en/1.1.x/}
% \bibitem[14]{pdoc} \emph{https://pypi.org/project/pdoc3/}
% \bibitem[15]{librosa} \emph{https://librosa.org/doc/latest/index.html}
% \bibitem[16]{chordpoints} Christoph Hausner, \emph{Design and Evolution of a~Simple Chord Detection Algorithm}, 2014.
% \bibitem[17]{paperAutomaticMusicTranscription} Piszczalski, Martin i~Bernard A. Galler,
% \emph {Automatic Music Transcription}, Computer Music Journal, vol. 1, no. 4, 1977, 24–31.
% \bibitem[18]{paperOnTheComputerRecognition} Dixon, Simon,
% \emph{On the Computer Recognition of Solo Piano Music}, 2003.
% \bibitem[19]{challengesAndFuture} E. Benetos, S. Dixon, D. Giannoulis
% \emph{Automatic music transcription: challenges and future directions}, Journal of Intelligent Information Systems, 2013, 407–434.
% \bibitem[20]{pianoTranscriptions} Sebastian Ewert, Mark B. Sandler,
% \emph{Piano Transcription in the Studio Using an Extensible Alternating Directions Framework}, 2016.
% \bibitem[21]{zdjecia} \emph{https://unsplash.com/}

\end{thebibliography}

\thispagestyle{empty}
\pagenumbering{gobble}



% ----- 8. Spis rysunków - jeśli nie ma, zakomentować --------
\listoffigures
\thispagestyle{empty}


% ------------ 9. Spis tabel - jak wyżej ------------------
% \renewcommand{\listtablename}{Spis tabel}
% \listoftables
% \thispagestyle{empty}


% 10. Spis załączników - jak nie ma załączników, to zakomentować lub usunąć

% \chapter*{Spis załączników}
% \begin{enumerate}[label=(\Alph*)]
% \item Instrukcja instalacji i obsługi
% \end{enumerate}

% \thispagestyle{empty}





\end{document}